\chapter{Motion Planning with Latent Human Intentions}\label{chap:hri}

Robots that are designed to assist humans almost inevitably have to operate in
a shared environment. \ac{hri} requires autonomous agents to navigate
safely in domains that typically do not feature safety barriers to physically
separate them from humans. Therefore, robots must ensure human safety through
careful planning and robust behaviors. At the same time, trajectories of humans
are hard to predict as they follow complex behaviors whose dynamics are only
partially understood.\todo{cite} For this reason, research in the past has
moved from simple rule-based and deterministic models \todo{social forces etc.}
towards data driven probabilistic predictions that approximate the future as
a distribution over trajectories \todo{cite: social lstm, and learning to
predict trajectories}.

\todo[inline]{Explain the notion of latent, internal state}

Incorporating stochasticity in the prediction pipeline allows the planner to
model uncertainty over both, high-level intentions (e.g. \emph{where} does the
human want to go), and low-level motion behavior (e.g. \emph{how} does the
human want to go there). Planning strategies that take this uncertainty into
account promise to provide robustified and potentially more efficient policies
for navigation around humans. While the properties gained from this kind of
planning are desirable for many \ac{hri} problems, reasoning over distributions
of possible futures may pose a challenging \ac{pomdp}. Therefore, a lot of
research has focused on recovering some of these properties by proposing domain
specific simplifications for these applications \cite{fern2007decision,
sadigh2016information, javdani2018shared, fisac2018probabilistically}.

On the other hand, recent research in this field suggests that through
increased performance of modern solvers, \ac{pomdp} approaches for motion
planning problems with \ac{hri} are becoming increasingly practical and that
this kind of reasoning can help to generate robustified and more efficient
behaviors for this domain. \cite{bai2015intention} use the \ac{despot} to
control the speed of an autonomous golf cart for navigation in a crowd. They
show that by reasoning over future observations the system is able to maneuver
more safely while reaching the goal faster and increasing passenger comfort
through smoother trajectories than the proposed greedy baseline.
\cite{sunberg2017value} examine the value of inferring the internal state of
traffic participants for autonomous freeway driving. The results presented in
their work show that the problem allows for a significant increase in
performance when the agent is omniscient to the individual internal state of
other vehicles compared to planning with a static \emph{normal} behavior for
all traffic participants. They demonstrate that inference of the latent
behavior parameters in combination with \ac{pomdp} planning allows to greatly
reduce the gap to the omniscient upper bound.

While a lot of work in motion planning under uncertainty has focus on either
\emph{problem specific simplifications} on the one hand or \emph{full
\ac{pomdp} solutions} on the other, few results exist on the direct comparison
of the two. In this chapter we consider a case of motion planning in the
presence of humans for which an elaborate domain specific strategy has been
proposed in \cite{fisac2018probabilistically}. This strategy simplifies the
planning problem by neglecting future observations. Instead, human prediction
are performed on expectation from the current belief, providing series of
probability maps for future human lotions. The authors propose a method that
allows planning with these probabilistic predictions by means of conventional
motion planning algorithms. We implement this strategy in the \pomdpsjl
framework to compare it's performance with behaviors generated by
\ac{pomcpow}.

We begin by stating the details of the motion planning problem in
\cref{sec:hri-problem-statement}. \cref{sec:hri-pomdp-formalization} then
formalizes this problem as a \ac{pomdp}. \cref{sec:hri-solutions} briefly
presents the domain specific approximate planner proposed in
\cite{fisac2018probabilistically}, as well as the \ac{pomcpow} solver adapted
for this problem. Finally, we evaluate the performance of both policies and
discuss the results in \cref{sec:hri-evaluation}.

\section{Problem Statement}\label{sec:hri-problem-statement}

In this chapter we study a motion planning problem for a robot in a shared
environment with a human actor. The assumptions made for this planning problem
aim to closely match the setup described in \cite{fisac2018probabilistically}.

\todo[inline]{add screenshot of simulator}

\paragraph{High Level Problem}

Consider a human and a robot navigating in a common environment. As a running
example, this problem may be envisioned as an indoor navigation problem. In
this problem, the robot aims to efficiently reach a preset goal location
inside the room while trying to avoid collisions with the human. At the same
time, the human does not pay attention to the robot. That is, the decisions of
the human actor do not depend on the location of the robot. Furthermore, the
human has time-varying intentions that are not known by the robot. At every
time step the human aims to reach a certain goal inside the room. Once arrived
at this goal, the human may either stay at this goal or proceed to another goal
location. Furthermore, the human has a small likelihood of changing the
intended walk target mid way. The robot has a model for some of the locations
humans might want to go to. However, the planner neither knows the exact path
the human will take, nor is it's model of human goals complete. That is, the
person might aim to reach an unmodeled goal location. At every time step the
robot receives a perfect observation of the current position of both itself and
the human. The robot ultimately needs to plan and execute a trajectory to
a goal state according to some notion of efficiency, while avoiding collisions
with the human.

\paragraph{Human Behavior Model}

This problem poses a challenging planning problem as it requires the agent to
carefully reason about the intentions of the human in order to avoid collisions
while trying to reach it's own goal. Furthermore, in order to be robust against
cases in which the person tries to approach an unmodeled goal, the agent needs
to make safe fall-back predictions when the human behaves unexpectedly. At the
same time, planning with the worst case assumption by trying to avoid the
humans forward reachable set is too conservative.

In this problem setting, the robot is assumed to have access to suitable human
reward function. In practice such reward model can be learned offline from
prior human demonstrations, e.g. using inverse reinforcement
learning\todo{cite?}. By means of a noisy-rationality model used in cognitive
science \cite{baker2007goal}, the planner can make probabilistic predictions
of human actions given the state,

\begin{equation}\label{eq:boltzmann}
  P\left(a^t_\text{H} | s^t_\text{H}; \beta, \theta \right) \propto e^{\beta Q_\text{H}\left(s^t_\text{H}, a^t_\text{H}; \theta\right)}.
\end{equation}

Here, $a^t_\text{H}$ is the human action taken at time $t$; $s^t_\text{H}$ is
the state of the human at this time; $Q_\text{H}(s_\text{H}, a_\text{H}; \theta)$ is the
state-action value function of the human, with $\theta$ being the parameters of the
$Q$-value encoding latent human intentions (e.g. the goal location); $\beta$ is
the so called \emph{rationality coefficient}.\\
This model encodes the assumption that humans are more likely to select actions that
provide a higher $Q$-value. The rationality constant, $\beta$, determines the
degree to which the robot expects the human to align with the provided reward
model. For $\beta = 0$ the model in \cref{eq:boltzmann} degenerates to
a uniform distribution over all actions. For $\beta \to \infty$ the human acts
perfectly rational with respect to the utility model,
$Q_\text{H}$.

Here, we treat both $\theta$ and $\beta$ as latent parameters. That is, the
agent needs to reason over the parameters of the human reward model (e.g. it's
current goal) as well as the accuracy of this model.

\section{POMDP Formalization}\label{sec:hri-pomdp-formalization}
\section{Solution Strategies}\label{sec:hri-solutions}
\subsection{Probabilistically Safe Robot Planning}\label{sec:hri-baseline}
\subsection{POMCPOW}\label{sec:hri-planners}
\section{Evaluation}\label{sec:hri-evaluation}
