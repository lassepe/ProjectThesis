\chapter{Simultaneous Localization and Planning}\label{chap:localization-and-planning}

\todo[inline]{Rework according to results}

Typically, robot localization and motion planning are treated as mostly
independent tasks. Often, the problem provides the agent with a focused initial
belief or allows the agent to quickly reduce uncertainty by first collecting
a series of measurements that don't require elaborate motion planning. This is
particularly the case if the state space is well observed, i.e. there exists
a sensor that provides the robot with a (noisy) position reading (e.g. GPS).
For these well behaved problems, a simple yet effective approach is to first
collect sensor data while executing a default motion, i.e. rotating about the
yaw axis to collect measurements in all directions. Once state uncertainty is
reduced to a focused, unimodal belief, the problem is treated as fully observed
problem and motion planning is performed on expectation. That is, trajectories
are planned based on the assumption that the expectation of the belief, $E[b]$,
is a sufficient approximation for the true position of the robot. Furthermore,
higher statistical moments \todo{What is the right word for this?} may be used
to trigger additional information gathering. In even simpler problems, plan
execution may implicitly yield sufficient information gathering, rendering
explicit consideration of this aspect obsolete.

However, more challenging problems are obtained when information gathering
happens less naturally. In these cases, a simple approach as outlined above
may not perform well. Instead, persistently good performance requires the agent
to consider the full belief distribution as well as future observations in
order to identify informative action sequences.

\todo[inline]{Rewrite once structure (below) has converged}
An instance of such problem is studied in this chapter. We begin by stating the
problem details in \cref{sec:lp-problem-statement}.
\cref{sec:lp-pomdp-formalization} then formalizes this problem as a \ac{pomdp}.
\cref{sec:lp-solutions} presents solution strategies based on \ac{pomdp}
solvers introduced in \cref{sec:online-pomdp-solvers} as well as two heuristic
baseline approaches. Finally, we evaluate the performance of each solver and
discuss the results \cref{sec:lp-evaluation}.

\section{Problem Statement}\label{sec:lp-problem-statement}

Consider a robot operating in a planar, previously mapped room as depicted in
\cref{fig:roomba-env}. In this figure the red marker denotes the true position
of the robot, while the blue particles represent the robot's belief of it's
current location in the room. The agent is tasked to reach the exit (green)
while avoiding falling down the stairs (red). Additionally, unnecessary
collisions are to be avoided. The scene shows the simulation in it's initial
configuration: The robot starts at a position drawn uniformly at random from
the set of all possible position-orientation tuples. This initial position is
not known by the agent, thus the particles representing the initial belief are
spread uniformly across the entire room. The robot is equipped with a single
sensor that provides information about collisions with walls of the room. The
robot's dynamics are governed by an underactuated first-order model that allows
the robot to translate forward and/or rotate about it's vertical axis. The
agent holds control authority to choose translation and rotation rates.

This problem has originally been published under the name \emph{"Escape
Roomba"} by the \emph{Stanford Intelligent Systems Laboratory} as part of the
class \emph{Decision Making under
Uncertainty}\footnote{\url{https://web.stanford.edu/class/aa228/cgi-bin/wp/}}.
The experiments presented in the following have been conducted using the
\pomdpsjl implementation of the
problem\footnote{\url{https://github.com/sisl/AA228FinalProject}}.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.7\textwidth]{roomba_demo/roomba-initial.png}
  \caption{A screen shot of the simulation environment for the \emph{"Escape
  Roomba"} problem provided by the \emph{Stanford Intelligent Systems
  Laboratory} as part of the class \emph{Decision Making under Uncertainty}.}
  \label{fig:roomba-env}
\end{figure}

\clearpage
\section{POMDP Formalization}\label{sec:lp-pomdp-formalization}

The problem is formalized as a \ac{pomdp} with the following properties introduced in \cref{sec:pomdp}:

\todo[inline]{Mention somewhere that we discretize the action space because
samples are expensive (problem setup is quite slow)}

\begin{description}
	\item[State Space $\sspace$.] The state of the robot is defined as the
	position, orientation tuple, $s=((x,y), \alpha)$. Respectively, $\sspace$ is
	the set of all positions-orientation tuples inside the room.
	\item[Action Space $\aspace$.] An action is defined as a translation-rotation
		rate tuple, $a=(v, \omega)$. $\aspace$ is the set of all these
		tuples adhering to the physical limitations of the system.
  \item[Transition Model $\tdist$.] The robot transition flows the dynamics of
    a simple differential drive model. The details of this transition are
    defined implicitly by the simulator and are treated as a generative black
    box model.
  \item[Reward Function $\reward: \sspace \times \aspace \times
    \sspace \to \reals$.] The reward model is defined as:
    \begin{equation}
      \reward(s, a, s') = r_\text{time} + \chrond(s') r_\text{collision} + \chrond(s') r_\text{goal} + \chrond(s') r_\text{stairs}.
    \end{equation}
    Here, $\chrond_\text{c}(s)$ denotes the \vname{Kronecke delta} function under the condition, \vname{c}, such that
    \begin{equation}
      \chrond_\text{c}(s) = \begin{cases}
        1, & \text{if condition $c$ holds for $s$}\\
        0, & \text{otherwise.}
      \end{cases}
    \end{equation}
    The reward model is parametrized by the following
    quantities: $r_\text{time}$ is a living penalty encouraging the robot to
    minimize the time until the goal is reached; $r_\text{collision}$ is
    a penalty for collision with walls; $r_\text{goal}$ is the reward obtained
    when reaching the goal, and $r_\text{stairs}$ the penalty for falling down
    the stairs.\\
  \item[Discount Factor $\gamma$.] Rewards are discounted with $\gamma = 0.99$.
  \item[Observation Space $\ospace$.] The sensor provides a binary output,
    stating whether the robot is currently in contact with the wall or not. That
    is, there are only two observations: $o^0$  and $o^1$.
  \item[Observation Model $\odist$.] The collision sensor is deterministic.
    That is, the observation model is a Dirac delta function, evaluating
    non-zero for a state, $s$, if and only if the collision feature evaluated
    on $s$ matches the observation, $o$.
\end{description}

Summarizing, the problem is a \ac{pomdp} with continuous state and action space
and a discrete observation space. The reward model chosen for this problem is
sparse. That is, it only encodes high level preferences but avoids biasing the
solution through additional transition dependent rewards (e.g. rewards for
reducing the distance to the goal).

\section{Solution Strategies}\label{sec:lp-solutions}

We solve the \ac{pomdp} described in \cref{sec:lp-pomdp-formalization} using
\ac{despot} and \ac{pomcpow}, respectively. For each of these solvers we
propose two domain specific adaptations to guide the policy search. In addition
to that, we present two baseline policies that use a heuristic policy to
control the agent. In order to consider efficiency of each algorithm we limit
the computation time per planning step to $T_\text{max} = \SI{1}{s}$. Each
policy uses the same particle filter to estimate the root belief.
\todo[inline]{Maybe point to code attached or to repo.}

\subsection{Baseline Policies}\label{sec:lp-baseline}

In order to obtain a baseline we make a common simplifying assumption: Instead
of planning with the full root belief, $b$, the policy uses only the \emph{most
likely state}, $\sml = \mode(b)$. Using this approximation, the problem is then
treated as fully observable. That is, at every time step the control action is
selected based on the assumption that the true position of the robot matches
the most likely state. The loop is closed by applying only the first action of
the policy at each time step to then update the belief and re-plan from the
next most likely state, $\sml' = \mode(b')$. Based on this assumption we
propose two heuristic policies:

\begin{description}
  \item[\ac{mlra}] For this baseline, the agent uses a simple, handcrafted
  feedback policy that does not involve any active reasoning about future
  states. Instead, it uses a proportional controller to track the center line
  of the room to reach exit. As a result, evaluating \ac{mlra} is
  computationally inexpensive. \ac{mlra} is not an optimal policy for the fully
  observable problem.
  \item[\ac{mlmpc}] This planning agent computes the optimal state trajectory
  for the fully observable problem starting from $\sml$. That is, it uses the
  negative reward model as a cost function to obtain the objective for the
  model predictive controller.
\end{description}

Unlike \ac{mlra}, \ac{mlmpc} is an optimal policy for the fully observed
problem. However, it should be noted that this result does not necessarily
translate to the partially observed domain. Appendix TODO\todo{cref to
appendix} compares the performance of both baseline policies for the fully
observable case.

\subsection{POMDP Solutions}\label{sec:lp-planners}

We use \ac{despot} and \ac{pomcpow} to solve the \ac{pomdp}. For each solver we
propose two strategies in which domain knowledge can be utilized to guide the
policy search for the localization and planning problem.

\subsubsection{DESPOT}\label{sec:lp-planners-despot}

As presented in \cref{sec:theory-despot}, \ac{despot} uses upper and lower
bounds, $U_0$ and $L_0$, on the value function to guide the policy search and
prune the determinized policy tree. In theory, the tighter these bounds are on
the true value function, the more efficiently the algorithm will focus on
exploring the relevant trajectories. On the other hand, if these estimates
violate the true bounds on the optimal cost to go, \ac{despot} with perform
sub-optimally. However, as proven in \cite{somani2013despot}, the algorithm is
robust against approximation errors. That is, similar to informed graph search
algorithms like \emph{A*}, the performance of the algorithm will degrade
gracefully with violations of $U_0$. In fact, similar to bounded relaxations of
\emph{A*}, non-admissible heuristics may help to find good solutions earlier.
Therefore, even if $U_0$ is not a true upper bound on the optimal value, it may
help to improve performance when limited planning time does not allow to
thoroughly explore the search space. Furthermore, it should be noted that
finding tight bounds at the expense of high compute also defeats the purpose.
In the limit, finding the tightest upper and lower bound on the optimal value
requires solving the entire \ac{pomdp}. Therefore, choosing suitable upper and
lower bounds is an important design choice when adapting \ac{despot} for
a specific problem.

There exist a variety of strategies to compute these upper and lower bounds.
\todo{maybe list some common ones}
In the following we present two of the strategies we found most effective for
the problem studied here.
\todo[inline]{point to the code on the cd (or appendix)}

\begin{description}
  \item[Analytic Bounds] In this strategy we compute the bounds using an
    analytic, closed form\todo{what is a better phrasing for this?}
    approximation for $U_0$ and $L_0$. Due to the structure of the reward
    model, we can directly compute the cumulative reward for an arbitrary
    $n$-step, collision-free trajectory. Let $\hat{\Sigma}: \naturals \to
    \reals$ be this mapping. We compute the upper bound, $U_0$, on the optimal
    value by considering a strict relaxation of the problem: First, we assume
    that the robot is located at the closest position to the goal out of all
    particles in the current belief; Second, we assume that the robot can
    immediately move on a straight line to the target. With this relaxation we
    compute the minimum remaining number of steps, $n_{\text{min}}$, from the
    minimum Euclidean distance over all particles to the goal and the maximum
    translational speed. For the lower bound, $L_0$, we proceed in
    a similar fashion. In this case, however, we consider the worst case
    particle by using the $\ell_1$ norm as a distance metric combined with an
    under-approximation on the velocity to obtain $n_{\text{max}}$. In either
    case, we use $\hat{\Sigma}$ to map the step estimate to a corresponding
    return. Using these approximations, $U_0$ is a true upper bound on the
    optimal value. While for $L_0$ there is no strict guarantee to be a true
    lower bound, we found that most cases it is well behaved.
  \item[Rollout Lower Bound] This strategy uses the same upper bound as the
    first approach. However, for $L_0$ we use a default policy rollout to
    obtain a true lower bound. In order to generate a structured motion, we use
    a \emph{fixed-action} policy that always selects the forward action,
    instead of the common random policy rollout. By this means, the rollout
    policy provides a tight lower bound once evaluated from a state that
    already faces the exit of the room.
\end{description}

\subsubsection{POMCPOW}\label{sec:lp-planners-pomcpow}

\ac{pomcpow} uses a value estimate to approximate the remaining reward to be
collected from a leave node of the policy tree. Due to the design of the
algorithm, this value estimate is only ever invoked on observation nodes when
visited for the first time (cf. \cref{alg:pomcpow}). As a result, the
corresponding belief consists of only a single state and the problem is de
facto fully observable. Hence, in order to adapt this algorithm for a specific
problem one can design the value estimate based on the corresponding \ac{mdp}.
Hereafter, we present two strategies strategies to compute this value estimate
for the localization and planning problem.

\begin{description}
  \item[Analytic Value Estimate] We compute an analytic value estimate for
    state, $s$, with a strategy similar to the one used to obtain analytic
    bounds for \ac{despot}. We relax the planning problem by assuming that the
    robot can move immediately from $s$ to the goal on a rectilinear
    trajectory. Using this relaxation we can compute the estimate on the
    remaining number of steps to the goal, $\hat{n}$. We then obtain the value
    estimate for an $\hat{n}$-step, collision-free trajectory by propagating
    $\hat{n}$ through $\hat{\Sigma}$, introduced above.
  \item[Rollout Estimate] A commonly used strategy in \ac{mcts} is a random
    policy rollout \cite{browne2012survey}. For this problem, however, random
    play-outs do not provide a good value estimate since random trajectories
    will often fail to reach a terminal state within reasonable time. Instead,
    we use the \emph{fixed-action} policy proposed for the \ac{despot} lower
    bound approximation in the preceding paragraph.
\end{description}

\section{Evaluation}\label{sec:lp-evaluation}

\begin{itemize}
  \item spending a lot of time to find tight bounds does not help
  \item POMCPOW: In the face of uncertainty it is good to be optimistic
\end{itemize}


\begin{figure}[ht]
  \begin{adjustbox}{addcode={\begin{minipage}{\width}}{\caption{%
  \todo[inline]{Describe and fix scaling and font}
      }\end{minipage}},rotate=90,center}
      \includegraphics[scale=0.5]{roomba_plots/data_all_plot.pdf}
  \end{adjustbox}
\end{figure}
