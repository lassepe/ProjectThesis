\chapter{Introduction}

\todo[inline]{General, broad introduction to Planning in Uncertain Environments
in the real world featuring some state of the art work.}.
\begin{itemize}
  \item why does considering uncertainty matter
  \item very brief introduction to POMDP which can be mainly copied from the project proposal
  \item why is it challenging to do it like this, why do people avoid this:
        point to planning on expectation and making certain assumptions on the structure to
        make the problem simpler (e.g. MOMDP etc.) or neglecting the fact that
        in the future the agent get additional information
  \item very brief summary what this work is going to show / what is the point
        of this work (just a very short paragraph). Details can be mentioned in the
        "motivation" section. (this can be refined at the end)
  \item chapter outline (also in the end, as the structure has converged)
\end{itemize}

\section{Motivation}

\todo[inline]{Motivate this work}

\begin{itemize}
  \item This work seeks to explore the use of POMDPs for Planning in Uncertain Environments
  \item looking at two popular problems (localization and motion planning)
  where partial observability is usually neglected or treated with significant
  simplifications, what is the performance gain that can be achieved.
\end{itemize}

\todo[inline]{Project proposal. Placed here as a template}
The \textit{partially observable Markov decision process} (POMDP) provides
a principled general framework for planning in partially observable stochastic
environments. In contrast to their well known fully observable counter part,
the \textit{Markov Decision Process} (MDP), in a POMDP the agent can not access
state information directly. Instead, the agent is presented with observations
as stochastic emissions of the true state of the world. Therefore, solving
a POMDP requires the planner to reason about a distribution over possible
futures with uncertainty in both the transition and observation model. This
makes solving a POMDP a significantly more challenging problem than solving the
corresponding fully observable MDP.\\

A POMDP is a PSPACE-complete problem and thus optimal solutions to problems of
this class can not be found in polynomial time. Therefore, in robotics
applications, characterized by limited compute and real time constraints, POMDP
solution methods are traditionally avoided. Instead, simplifications are made
that neglect the partial observability or make other assumptions about the
problem structure.\\

This project aims to explore how despite these challenges the POMDP
framework can be used to compute robust policies for a robot to optimally
interact with an uncertain environment. For this purpose, uncertainty in
two different subsets of the state space are considered:\\
\textit{External States,}  states of the environment. (e.g. position of the
robot).\\
\textit{Internal States,} latent \textit{model parameters} of the
environment (e.g. intentions of other agents).\\

The use of POMDP solution methods for planning in the face of uncertainty in
each of these domains will be examined. External state uncertainty will be
studied in a simulated environment at the example of simultaneous localization
and planning. Internal state uncertainty will be studied at the example of
human robot interaction.\\
In order to address the issue of limited compute in the context of robotics
applications, this project will investigate how meta reasoning can be employed to
dynamically switch models during on-line planning in an effort to account for
the trade-off between accuracy and computational complexity when reasoning about
future environment states.
